name: Terraform Destroy Workflow
on:
  workflow_dispatch:

permissions:
  id-token: write
  contents: read
  actions: read

jobs:
  terraform-destroy:
    name: Terraform Destroy
    runs-on: ubuntu-latest
    environment: production
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v5
        
      - name: Verify Variables Available
        run: |
          echo "CLUSTER_NAME: ${{ vars.CLUSTER_NAME }}"
          echo "NAMESPACE: ${{ vars.APP_NAMESPACE }}"
          echo "MONITORING_NAMESPACE: ${{ vars.MONITORING_NAMESPACE }}"
          echo "ARGOCD_NAMESPACE: ${{ vars.ARGOCD_NAMESPACE }}"
          echo "APP_NAME: ${{ vars.APP_NAME }}"
          echo "KARPENTER_NODEPOOL_NAME: ${{ vars.KARPENTER_NODEPOOL_NAME }}"
          echo "KARPENTER_NODECLASS_NAME: ${{ vars.KARPENTER_NODECLASS_NAME }}"
          echo "KARPENTER_NODE_ROLE: ${{ vars.KARPENTER_NODE_ROLE }}"
          echo "KARPENTER_INSTANCE_PROFILE: ${{ vars.KARPENTER_INSTANCE_PROFILE }}"
          echo "KARPENTER_NAMESPACE: ${{ vars.KARPENTER_NAMESPACE }}"
          if [[ -z "${{ vars.CLUSTER_NAME }}" ]]; then
            echo "ERROR: CLUSTER_NAME variable not found. Infrastructure may not be deployed."
            exit 1
          fi
          if [[ -z "${{ vars.APP_NAMESPACE }}" ]]; then
            echo "ERROR: APP_NAMESPACE variable not found. Infrastructure may not be deployed."
            exit 1
          fi
        
      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsInfraRole
          aws-region: us-east-1
          
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3.1.2
        with:
          terraform_version: 1.5.7
          
      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name ${{ vars.CLUSTER_NAME }} --region us-east-1
        continue-on-error: true
        
      - name: Install Helm
        uses: azure/setup-helm@v4.2.0
        with:
          version: v3.14.0
        continue-on-error: true

      # ---------------------------
      # Delete ArgoCD Applications with timeout
      # ---------------------------
      - name: Delete ArgoCD Applications
        run: |
          kubectl delete application ${{ vars.APP_NAME }} -n ${{ vars.ARGOCD_NAMESPACE }} --ignore-not-found --timeout=60s || true
          kubectl delete application kube-prometheus-stack -n ${{ vars.ARGOCD_NAMESPACE }} --ignore-not-found --timeout=60s || true
        continue-on-error: true

      # ---------------------------
      # Scale down workloads FIRST with timeouts
      # ---------------------------
      - name: Scale down all workloads before cleanup
        run: |
          echo "Scaling down all deployments and deleting services to trigger LB cleanup..."
          
          # Scale down ALL deployments across ALL namespaces with timeout
          kubectl get deployments --all-namespaces -o json | jq -r '.items[] | "\(.metadata.namespace) \(.metadata.name)"' | while read namespace deployment; do
            echo "Scaling down deployment $deployment in namespace $namespace"
            kubectl scale deployment $deployment --replicas=0 -n $namespace --timeout=30s || true
          done
          
          # Scale down daemonsets that might be running
          kubectl get daemonsets --all-namespaces -o json | jq -r '.items[] | "\(.metadata.namespace) \(.metadata.name)"' | while read namespace daemonset; do
            echo "Deleting daemonset $daemonset in namespace $namespace"
            kubectl delete daemonset $daemonset -n $namespace --ignore-not-found --timeout=60s || true
          done
          
          # Delete ALL services of type LoadBalancer IMMEDIATELY
          echo "Deleting LoadBalancer services..."
          kubectl get services --all-namespaces -o json | jq -r '.items[] | select(.spec.type=="LoadBalancer") | "\(.metadata.namespace) \(.metadata.name)"' | while read namespace service; do
            echo "Deleting service $service in namespace $namespace"
            kubectl delete service $service -n $namespace --ignore-not-found --timeout=60s || true
          done
          
          # Wait for LoadBalancers to be cleaned up
          echo "Waiting for LoadBalancers to be cleaned up..."
          sleep 60
      
      # ---------------------------
      # Clean up Karpenter Resources with enhanced error handling
      # ---------------------------
      - name: Delete Karpenter Provisioners and AWSNodeTemplates
        run: |
          echo "Deleting Karpenter Provisioners..."
          kubectl delete provisioner ${{ vars.KARPENTER_NODEPOOL_NAME }} --ignore-not-found -n ${{ vars.KARPENTER_NAMESPACE }} --timeout=60s || true
          kubectl delete provisioner --all -n ${{ vars.KARPENTER_NAMESPACE }} --ignore-not-found --timeout=60s || true
          
          echo "Deleting Karpenter AWSNodeTemplates..."
          kubectl delete awsnodetemplate ${{ vars.KARPENTER_NODECLASS_NAME }} --ignore-not-found -n ${{ vars.KARPENTER_NAMESPACE }} --timeout=60s || true
          kubectl delete awsnodetemplate --all -n ${{ vars.KARPENTER_NAMESPACE }} --ignore-not-found --timeout=60s || true
          
          echo "Deleting new Karpenter resources (NodePools, EC2NodeClasses)..."
          kubectl delete nodepool --all --ignore-not-found --timeout=60s || true
          kubectl delete ec2nodeclass --all --ignore-not-found --timeout=60s || true
          
          echo "Waiting for resources to be cleaned up..."
          sleep 30
        continue-on-error: true

      - name: Uninstall Karpenter Helm Release
        run: |
          echo "Uninstalling Karpenter Helm release..."
          helm uninstall karpenter -n ${{ vars.KARPENTER_NAMESPACE }} --timeout=300s || true
          
          echo "Waiting for pods to terminate..."
          kubectl wait --for=delete pod -l app.kubernetes.io/name=karpenter -n ${{ vars.KARPENTER_NAMESPACE }} --timeout=120s || true
          
          echo "Force deleting any remaining pods..."
          kubectl delete pods --all -n ${{ vars.KARPENTER_NAMESPACE }} --force --grace-period=0 || true
        continue-on-error: true

      - name: Clean up Karpenter CRDs and Webhooks
        run: |
          echo "Deleting Karpenter CRDs..."
          kubectl delete crd provisioners.karpenter.sh --ignore-not-found --timeout=60s || true
          kubectl delete crd awsnodetemplates.karpenter.k8s.aws --ignore-not-found --timeout=60s || true
          kubectl delete crd nodepools.karpenter.sh --ignore-not-found --timeout=60s || true
          kubectl delete crd ec2nodeclasses.karpenter.k8s.aws --ignore-not-found --timeout=60s || true
          
          echo "Deleting Karpenter webhooks..."
          kubectl delete validatingwebhookconfiguration defaulting.webhook.karpenter.sh --ignore-not-found || true
          kubectl delete validatingwebhookconfiguration validation.webhook.karpenter.sh --ignore-not-found || true
          kubectl delete mutatingwebhookconfiguration defaulting.webhook.karpenter.sh --ignore-not-found || true
          
          echo "Removing finalizers from stuck CRDs..."
          kubectl patch crd provisioners.karpenter.sh -p '{"metadata":{"finalizers":[]}}' --type=merge || true
          kubectl patch crd awsnodetemplates.karpenter.k8s.aws -p '{"metadata":{"finalizers":[]}}' --type=merge || true
          kubectl patch crd nodepools.karpenter.sh -p '{"metadata":{"finalizers":[]}}' --type=merge || true
          kubectl patch crd ec2nodeclasses.karpenter.k8s.aws -p '{"metadata":{"finalizers":[]}}' --type=merge || true
        continue-on-error: true

      # ---------------------------
      # Uninstall Other Helm Releases with timeouts
      # ---------------------------
      - name: Uninstall Helm Releases
        run: |
          helm uninstall ${{ vars.APP_NAME }} -n ${{ vars.APP_NAMESPACE }} --timeout=300s || true
          helm uninstall kube-prometheus-stack -n ${{ vars.MONITORING_NAMESPACE }} --timeout=300s || true
          helm uninstall ingress-nginx -n ingress-nginx --timeout=300s || true
          helm uninstall argocd -n ${{ vars.ARGOCD_NAMESPACE }} --timeout=300s || true
        continue-on-error: true

      # ---------------------------
      # Delete CRDs (Prometheus & Grafana) before namespace deletion
      # ---------------------------
      - name: Delete Monitoring CRDs
        run: |
          echo "Deleting monitoring CRDs..."
          kubectl get crd -o name | grep -E 'prometheus|grafana|alertmanager|servicemonitor|prometheusrule' | xargs -r kubectl delete --timeout=60s || true
          
          echo "Deleting ArgoCD CRDs..."
          kubectl get crd -o name | grep 'argoproj.io' | xargs -r kubectl delete --timeout=60s || true
        continue-on-error: true

      # ---------------------------
      # Cleanup PVCs & PVs before namespace deletion
      # ---------------------------
      - name: Cleanup Persistent Storage
        run: |
          echo "Deleting PVCs..."
          kubectl delete pvc --all -A --timeout=120s || true
          echo "Deleting PVs..."
          kubectl delete pv --all --timeout=120s || true
        continue-on-error: true

      # ---------------------------
      # Delete Namespaces with FORCE cleanup for stuck ones
      # ---------------------------
      - name: Delete Namespaces with Force Cleanup
        run: |
          echo "Deleting namespaces with proper cleanup..."
          
          # Function to force delete a namespace if it gets stuck
          force_delete_namespace() {
            local ns=$1
            echo "Processing namespace: $ns"
            
            if kubectl get namespace $ns --ignore-not-found 2>/dev/null; then
              # Try normal deletion first with timeout
              kubectl delete namespace $ns --ignore-not-found --timeout=120s || {
                echo "Normal deletion failed for $ns, trying force deletion..."
                
                # Remove finalizers and force delete
                kubectl get namespace $ns -o json | \
                  jq '.spec.finalizers = []' | \
                  kubectl replace --raw "/api/v1/namespaces/$ns/finalize" -f - || true
                
                # Wait a moment
                sleep 10
                
                # Verify deletion
                if kubectl get namespace $ns --ignore-not-found 2>/dev/null; then
                  echo "WARNING: Namespace $ns still exists after force deletion"
                else
                  echo "Successfully force deleted namespace $ns"
                fi
              }
            else
              echo "Namespace $ns does not exist"
            fi
          }
          
          # Delete namespaces one by one with force cleanup
          for ns in ${{ vars.APP_NAMESPACE }} ${{ vars.MONITORING_NAMESPACE }} ${{ vars.ARGOCD_NAMESPACE }} ingress-nginx ${{ vars.KARPENTER_NAMESPACE }}; do
            force_delete_namespace $ns
          done
          
          echo "Final namespace check:"
          kubectl get namespaces || true
        continue-on-error: true

      # ---------------------------
      # Wait for cleanup to complete
      # ---------------------------
      - name: Wait for cleanup and verify
        run: |
          echo "Waiting for cleanup to complete..."
          sleep 60
          
          echo "Verifying cleanup..."
          kubectl get pods -n ${{ vars.KARPENTER_NAMESPACE }} 2>/dev/null || echo "Karpenter namespace not found (expected)"
          kubectl get crd | grep karpenter || echo "No Karpenter CRDs found (expected)"
          kubectl get namespaces | grep -E "${{ vars.APP_NAMESPACE }}|${{ vars.MONITORING_NAMESPACE }}|${{ vars.ARGOCD_NAMESPACE }}" || echo "Target namespaces deleted (expected)"

      # ---------------------------
      # Terraform Destroy
      # ---------------------------
      - name: Terraform Init
        run: terraform init
        working-directory: ./Terraform
        
      - name: Terraform Destroy Plan
        run: terraform plan -destroy
        working-directory: ./Terraform
        
      - name: Terraform Destroy
        run: terraform destroy -auto-approve
        working-directory: ./Terraform

      # ---------------------------
      # Clean up GitHub Variables
      # ---------------------------
      - name: Remove GitHub repository variables
        run: |
          gh variable delete CLUSTER_NAME --repo $GITHUB_REPOSITORY || true
          gh variable delete APP_NAMESPACE --repo $GITHUB_REPOSITORY || true
          gh variable delete MONITORING_NAMESPACE --repo $GITHUB_REPOSITORY || true
          gh variable delete ARGOCD_NAMESPACE --repo $GITHUB_REPOSITORY || true
          gh variable delete APP_NAME --repo $GITHUB_REPOSITORY || true
          gh variable delete KARPENTER_NODEPOOL_NAME --repo $GITHUB_REPOSITORY || true
          gh variable delete KARPENTER_NODECLASS_NAME --repo $GITHUB_REPOSITORY || true
          gh variable delete KARPENTER_NODE_ROLE --repo $GITHUB_REPOSITORY || true
          gh variable delete KARPENTER_INSTANCE_PROFILE --repo $GITHUB_REPOSITORY || true
          gh variable delete KARPENTER_NAMESPACE --repo $GITHUB_REPOSITORY || true
        env:
          GITHUB_TOKEN: ${{ secrets.PAT_GITHUB }}
        continue-on-error: true
