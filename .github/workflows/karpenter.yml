name: Karpenter Deployment
on:
  workflow_dispatch:
  workflow_call:
    inputs:
      cluster_name:
        description: 'EKS Cluster Name'
        required: true
        type: string
      karpenter_nodepool_name:
        description: 'Karpenter NodePool Name'
        required: true
        type: string
      karpenter_nodeclass_name:
        description: 'Karpenter EC2NodeClass Name'
        required: true
        type: string
      karpenter_node_role:
        description: 'Karpenter Node IAM Role'
        required: true
        type: string
      karpenter_instance_profile:
        description: 'Karpenter Instance Profile'
        required: true
        type: string
      karpenter_namespace:
        description: 'Kubernetes namespace for Karpenter'
        required: true
        type: string
      karpenter_controller_cpu_request:
        description: 'CPU request for Karpenter controller'
        required: true
        type: string
      karpenter_controller_memory_request:
        description: 'Memory request for Karpenter controller'
        required: true
        type: string
      karpenter_controller_cpu_limit:
        description: 'CPU limit for Karpenter controller'
        required: true
        type: string
      karpenter_controller_memory_limit:
        description: 'Memory limit for Karpenter controller'
        required: true
        type: string

jobs:
  karpenter:
    name: Karpenter Installation & Configuration
    runs-on: ubuntu-latest
    environment: production

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v5

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsInfraRole
          aws-region: us-east-1

      - name: Update kubeconfig
        run: aws eks update-kubeconfig --name ${{ inputs.cluster_name }} --region us-east-1

      - name: Tag Resources for Karpenter Discovery
        run: |
          CLUSTER_NAME=${{ inputs.cluster_name }}
          
          # Tag cluster security groups
          SECURITY_GROUP_IDS=$(aws eks describe-cluster --name $CLUSTER_NAME --query 'cluster.resourcesVpcConfig.securityGroupIds' --output text)
          for sg_id in $SECURITY_GROUP_IDS; do
            aws ec2 create-tags --resources $sg_id --tags Key=karpenter.sh/discovery,Value=$CLUSTER_NAME
          done
          
          # Tag additional security groups with cluster name
          ADDITIONAL_SG_IDS=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=*$CLUSTER_NAME*" --query 'SecurityGroups[*].GroupId' --output text)
          for sg_id in $ADDITIONAL_SG_IDS; do
            aws ec2 create-tags --resources $sg_id --tags Key=karpenter.sh/discovery,Value=$CLUSTER_NAME
          done
          
          # Tag subnets
          SUBNET_IDS=$(aws eks describe-cluster --name $CLUSTER_NAME --query 'cluster.resourcesVpcConfig.subnetIds' --output text)
          for subnet_id in $SUBNET_IDS; do
            aws ec2 create-tags --resources $subnet_id --tags Key=karpenter.sh/discovery,Value=$CLUSTER_NAME
          done

      - name: Install Helm
        uses: azure/setup-helm@v4.2.0
        with:
          version: v3.14.0

      - name: Add Karpenter Helm Repo
        run: |
          helm repo add karpenter https://charts.karpenter.sh/
          helm repo update

      - name: Install/Upgrade Karpenter
        run: |
          CLUSTER_ENDPOINT=$(aws eks describe-cluster --name ${{ inputs.cluster_name }} --query 'cluster.endpoint' --output text)
          export CLUSTER_ENDPOINT=$CLUSTER_ENDPOINT
          echo "Cluster endpoint: $CLUSTER_ENDPOINT"
          
          # Use the working CLI approach with direct environment variables
          helm upgrade --install karpenter karpenter/karpenter \
            --version "0.16.3" \
            --namespace karpenter \
            --create-namespace \
            --timeout 15m \
            --set controller.env[0].name="CLUSTER_NAME" \
            --set controller.env[0].value="${{ inputs.cluster_name }}" \
            --set controller.env[1].name="CLUSTER_ENDPOINT" \
            --set controller.env[1].value="${CLUSTER_ENDPOINT}" \
            --set controller.env[2].name="AWS_DEFAULT_INSTANCE_PROFILE" \
            --set controller.env[2].value="${{ inputs.karpenter_instance_profile }}" \
            --set controller.env[3].name="INTERRUPTION_QUEUE" \
            --set controller.env[3].value="karpenter-interruption-queue-${{ inputs.cluster_name }}" \
            --set controller.env[4].name="AWS_REGION" \
            --set controller.env[4].value="us-east-1" \
            --set serviceAccount.annotations."eks\.amazonaws\.com/role-arn"="arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/KarpenterControllerRole-${{ inputs.cluster_name }}" \
            --set controller.resources.requests.cpu=${{ inputs.karpenter_controller_cpu_request }} \
            --set controller.resources.requests.memory=${{ inputs.karpenter_controller_memory_request }} \
            --set controller.resources.limits.cpu=${{ inputs.karpenter_controller_cpu_limit }} \
            --set controller.resources.limits.memory=${{ inputs.karpenter_controller_memory_limit }} \
            --wait

      - name: Wait for Karpenter to be Ready
        run: |
          echo "Waiting for Karpenter controller to be ready..."
          kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=karpenter -n karpenter --timeout=300s
          
          echo "Checking available Karpenter CRDs..."
          kubectl get crd | grep karpenter || echo "No Karpenter CRDs found yet"
          
          echo "Waiting for CRDs to be available..."
          kubectl wait --for condition=established --timeout=60s crd/awsnodetemplates.karpenter.k8s.aws || echo "AWSNodeTemplate CRD not ready"
          kubectl wait --for condition=established --timeout=60s crd/provisioners.karpenter.sh || echo "Provisioner CRD not ready"
              
          echo "Karpenter controller is ready!"

      - name: Deploy Karpenter NodePool and EC2NodeClass
        run: |
          export KARPENTER_NODEPOOL_NAME=${{ inputs.karpenter_nodepool_name }}
          export KARPENTER_NODECLASS_NAME=${{ inputs.karpenter_nodeclass_name }}
          export KARPENTER_NODE_ROLE=${{ inputs.karpenter_node_role }}
          export KARPENTER_INSTANCE_PROFILE=${{ inputs.karpenter_instance_profile }}
          export KARPENTER_NAMESPACE=${{ inputs.karpenter_namespace }}
          export CLUSTER_NAME=${{ inputs.cluster_name }}
          
          echo "Applying Karpenter resources..."
          envsubst < ./karpenter/karpenter-resources.yml | kubectl apply -f -

      - name: Verify Karpenter Installation
        run: |
          echo "Checking Karpenter controller status..."
          kubectl get pods -n karpenter
          
          echo "Checking Provisioner..."
          kubectl get provisioner
          
          echo "Checking AWSNodeTemplate..."
          kubectl get awsnodetemplate
          
          echo "Karpenter installation completed successfully!"